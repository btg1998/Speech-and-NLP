{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.2.5)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.11.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required parts from nltk parse\n",
    "from nltk.parse import DependencyGraph, DependencyEvaluator, ParserI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the other requred header files\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tempfile\n",
    "import pickle\n",
    "\n",
    "from os import remove\n",
    "from copy import deepcopy\n",
    "from operator import itemgetter\n",
    "\n",
    "from numpy import array\n",
    "from scipy import sparse\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Features\n",
    "class Configuration(object):\n",
    "    \"\"\"\n",
    "    Class for holding configuration which is the partial analysis of the input sentence.\n",
    "    The transition based parser aims at finding set of operators that transfer the initial\n",
    "    configuration to the terminal configuration.\n",
    "\n",
    "    The configuration includes:\n",
    "        - Stack: for storing partially proceeded words\n",
    "        - Buffer: for storing remaining input words\n",
    "        - Set of arcs: for storing partially built dependency tree\n",
    "\n",
    "    This class also provides a method to represent a configuration as list of features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dep_graph):\n",
    "        \"\"\"\n",
    "        :param dep_graph: the representation of an input in the form of dependency graph.\n",
    "        :type dep_graph: DependencyGraph where the dependencies are not specified.\n",
    "        \"\"\"\n",
    "        # dep_graph.nodes contain list of token for a sentence\n",
    "        self.stack = [0]  # The root element\n",
    "        self.buffer = list(range(1, len(dep_graph.nodes)))  # The rest is in the buffer\n",
    "        self.arcs = []  # empty set of arc\n",
    "        self._tokens = dep_graph.nodes\n",
    "        self._max_address = len(self.buffer)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Stack : ' + \\\n",
    "            str(self.stack) + '  Buffer : ' + str(self.buffer) + '   Arcs : ' + str(self.arcs)\n",
    "\n",
    "    def _check_informative(self, feat, flag=False):\n",
    "        \"\"\"\n",
    "        Check whether a feature is informative\n",
    "        The flag control whether \"_\" is informative or not\n",
    "        \"\"\"\n",
    "        if feat is None:\n",
    "            return False\n",
    "        if feat == '':\n",
    "            return False\n",
    "        if flag is False:\n",
    "            if feat == '_':\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def extract_features(self):\n",
    "        \"\"\"\n",
    "        Extract the set of features for the current configuration. Implement standard features as describe in\n",
    "        Table 3.2 (page 31) in Dependency Parsing book by Sandra Kubler, Ryan McDonal, Joakim Nivre.\n",
    "        Please note that these features are very basic.\n",
    "        :return: list(str)\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # Todo : can come up with more complicated features set for better\n",
    "        # performance.\n",
    "        if len(self.stack) > 0:\n",
    "            # Stack 0\n",
    "            stack_idx0 = self.stack[len(self.stack) - 1]\n",
    "            token = self._tokens[stack_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('STK_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('STK_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('STK_0_POS_' + token['tag'])\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('STK_0_FEATS_' + feat)\n",
    "            # Stack 1\n",
    "            if len(self.stack) > 1:\n",
    "                stack_idx1 = self.stack[len(self.stack) - 2]\n",
    "                token = self._tokens[stack_idx1]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('STK_1_POS_' + token['tag'])\n",
    "\n",
    "            # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == stack_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('STK_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('STK_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        # Check Buffered 0\n",
    "        if len(self.buffer) > 0:\n",
    "            # Buffer 0\n",
    "            buffer_idx0 = self.buffer[0]\n",
    "            token = self._tokens[buffer_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('BUF_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('BUF_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('BUF_0_POS_' + token['tag'])\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('BUF_0_FEATS_' + feat)\n",
    "            # Buffer 1\n",
    "            if len(self.buffer) > 1:\n",
    "                buffer_idx1 = self.buffer[1]\n",
    "                token = self._tokens[buffer_idx1]\n",
    "                if self._check_informative(token['word'], True):\n",
    "                    result.append('BUF_1_FORM_' + token['word'])\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_1_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 2:\n",
    "                buffer_idx2 = self.buffer[2]\n",
    "                token = self._tokens[buffer_idx2]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_2_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 3:\n",
    "                buffer_idx3 = self.buffer[3]\n",
    "                token = self._tokens[buffer_idx3]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_3_POS_' + token['tag'])\n",
    "                    # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == buffer_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('BUF_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('BUF_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(object):\n",
    "    \"\"\"\n",
    "    This class defines a set of transition which is applied to a configuration to get another configuration\n",
    "    Note that for different parsing algorithm, the transition is different.\n",
    "    \"\"\"\n",
    "    # Define set of transitions\n",
    "    LEFT_ARC = 'LEFTARC'\n",
    "    RIGHT_ARC = 'RIGHTARC'\n",
    "    SHIFT = 'SHIFT'\n",
    "    REDUCE = 'REDUCE'\n",
    "\n",
    "    def __init__(self, alg_option):\n",
    "        \"\"\"\n",
    "        :param alg_option: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type alg_option: str\n",
    "        \"\"\"\n",
    "        self._algo = alg_option\n",
    "        if alg_option not in [\n",
    "                TransitionParser.ARC_STANDARD,\n",
    "                TransitionParser.ARC_EAGER]:\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (TransitionParser.ARC_STANDARD, TransitionParser.ARC_EAGER))\n",
    "\n",
    "    def left_arc(self, conf, relation):\n",
    "        \"\"\"\n",
    "        Note that the algorithm for left-arc is quite similar except for precondition for both arc-standard and arc-eager\n",
    "            :param configuration: is the current configuration\n",
    "            :return : A new configuration or -1 if the pre-condition is not satisfied\n",
    "        \"\"\"\n",
    "        if (len(conf.buffer) <= 0) or (len(conf.stack) <= 0):\n",
    "            return -1\n",
    "        if conf.buffer[0] == 0:\n",
    "            # here is the Root element\n",
    "            return -1\n",
    "\n",
    "        idx_wi = conf.stack[len(conf.stack) - 1]\n",
    "\n",
    "        flag = True\n",
    "        if self._algo == TransitionParser.ARC_EAGER:\n",
    "            for (idx_parent, r, idx_child) in conf.arcs:\n",
    "                if idx_child == idx_wi:\n",
    "                    flag = False\n",
    "\n",
    "        if flag:\n",
    "            conf.stack.pop()\n",
    "            idx_wj = conf.buffer[0]\n",
    "            conf.arcs.append((idx_wj, relation, idx_wi))\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "\n",
    "    def right_arc(self, conf, relation):\n",
    "        \"\"\"\n",
    "        Note that the algorithm for right-arc is DIFFERENT for arc-standard and arc-eager\n",
    "            :param configuration: is the current configuration\n",
    "            :return : A new configuration or -1 if the pre-condition is not satisfied\n",
    "        \"\"\"\n",
    "        if (len(conf.buffer) <= 0) or (len(conf.stack) <= 0):\n",
    "            return -1\n",
    "        if self._algo == TransitionParser.ARC_STANDARD:\n",
    "            idx_wi = conf.stack.pop()\n",
    "            idx_wj = conf.buffer[0]\n",
    "            conf.buffer[0] = idx_wi\n",
    "            conf.arcs.append((idx_wi, relation, idx_wj))\n",
    "        else:  # arc-eager\n",
    "            idx_wi = conf.stack[len(conf.stack) - 1]\n",
    "            idx_wj = conf.buffer.pop(0)\n",
    "            conf.stack.append(idx_wj)\n",
    "            conf.arcs.append((idx_wi, relation, idx_wj))\n",
    "\n",
    "\n",
    "    def reduce(self, conf):\n",
    "        \"\"\"\n",
    "        Note that the algorithm for reduce is only available for arc-eager\n",
    "            :param configuration: is the current configuration\n",
    "            :return : A new configuration or -1 if the pre-condition is not satisfied\n",
    "        \"\"\"\n",
    "\n",
    "        if self._algo != TransitionParser.ARC_EAGER:\n",
    "            return -1\n",
    "        if len(conf.stack) <= 0:\n",
    "            return -1\n",
    "\n",
    "        idx_wi = conf.stack[len(conf.stack) - 1]\n",
    "        flag = False\n",
    "        for (idx_parent, r, idx_child) in conf.arcs:\n",
    "            if idx_child == idx_wi:\n",
    "                flag = True\n",
    "        if flag:\n",
    "            conf.stack.pop()  # reduce it\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def shift(self, conf):\n",
    "        \"\"\"\n",
    "        Note that the algorithm for shift is the SAME for arc-standard and arc-eager\n",
    "            :param configuration: is the current configuration\n",
    "            :return : A new configuration or -1 if the pre-condition is not satisfied\n",
    "        \"\"\"\n",
    "        if len(conf.buffer) <= 0:\n",
    "            return -1\n",
    "        idx_wi = conf.buffer.pop(0)\n",
    "        conf.stack.append(idx_wi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            \n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            \n",
    "            #model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            #model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhargav/.local/lib/python3.5/site-packages/nltk/parse/dependencygraph.py:380: UserWarning: The graph doesn't contain a node that depends on the root element.\n",
      "  \"The graph doesn't contain a node \"\n"
     ]
    }
   ],
   "source": [
    "# Loading the Train Data\n",
    "f = DependencyGraph.load('hi-ud-train.conllu')\n",
    "# 500 Training Examples\n",
    "f=f[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhargav/.local/lib/python3.5/site-packages/nltk/parse/dependencygraph.py:380: UserWarning: The graph doesn't contain a node that depends on the root element.\n",
      "  \"The graph doesn't contain a node \"\n"
     ]
    }
   ],
   "source": [
    "# Loading the Test Data\n",
    "f1 = DependencyGraph.load('hi-ud-test.conllu')\n",
    "# 100 Training Examples\n",
    "f1=f1[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x7f4ec1850268>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'ROOT': [], 'root': [9]}),\n",
      "                 'feats': None,\n",
      "                 'head': None,\n",
      "                 'lemma': None,\n",
      "                 'rel': None,\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'DET',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': 'Case=Nom|Number=Sing|Person=3|PronType=Dem',\n",
      "                 'head': 2,\n",
      "                 'lemma': 'yaha',\n",
      "                 'rel': 'det',\n",
      "                 'tag': 'DET',\n",
      "                 'word': 'yaha'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'PROPN',\n",
      "                 'deps': defaultdict(<class 'list'>, {'det': [1], 'case': [3]}),\n",
      "                 'feats': 'Case=Acc|Gender=Masc|Number=Sing|Person=3',\n",
      "                 'head': 6,\n",
      "                 'lemma': 'eSiyA',\n",
      "                 'rel': 'nmod',\n",
      "                 'tag': 'PROPN',\n",
      "                 'word': 'eSiyA'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': 'ADP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': 'AdpType=Post|Case=Acc|Gender=Fem|Number=Plur',\n",
      "                 'head': 2,\n",
      "                 'lemma': 'kA',\n",
      "                 'rel': 'case',\n",
      "                 'tag': 'ADP',\n",
      "                 'word': 'kI'},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'ADV',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': 'AdvType=Deg',\n",
      "                 'head': 5,\n",
      "                 'lemma': 'sabase',\n",
      "                 'rel': 'advmod',\n",
      "                 'tag': 'ADV',\n",
      "                 'word': 'sabase'},\n",
      "             5: {'address': 5,\n",
      "                 'ctag': 'ADJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {'advmod': [4]}),\n",
      "                 'feats': 'Case=Acc|Gender=Fem|Number=Plur',\n",
      "                 'head': 6,\n",
      "                 'lemma': 'badZA',\n",
      "                 'rel': 'amod',\n",
      "                 'tag': 'ADJ',\n",
      "                 'word': 'badZI'},\n",
      "             6: {'address': 6,\n",
      "                 'ctag': 'NOUN',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'amod': [5],\n",
      "                                      'case': [7, 8],\n",
      "                                      'nmod': [2]}),\n",
      "                 'feats': 'Case=Acc|Gender=Fem|Number=Plur|Person=3',\n",
      "                 'head': 9,\n",
      "                 'lemma': 'masjixa',\n",
      "                 'rel': 'nmod',\n",
      "                 'tag': 'NOUN',\n",
      "                 'word': 'masjixoM'},\n",
      "             7: {'address': 7,\n",
      "                 'ctag': 'ADP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': 'AdpType=Post',\n",
      "                 'head': 6,\n",
      "                 'lemma': 'meM',\n",
      "                 'rel': 'case',\n",
      "                 'tag': 'ADP',\n",
      "                 'word': 'meM'},\n",
      "             8: {'address': 8,\n",
      "                 'ctag': 'ADP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': 'AdpType=Post',\n",
      "                 'head': 6,\n",
      "                 'lemma': 'se',\n",
      "                 'rel': 'case',\n",
      "                 'tag': 'ADP',\n",
      "                 'word': 'se'},\n",
      "             9: {'address': 9,\n",
      "                 'ctag': 'NUM',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'cop': [10],\n",
      "                                      'nmod': [6],\n",
      "                                      'punct': [11]}),\n",
      "                 'feats': 'NumType=Card',\n",
      "                 'head': 0,\n",
      "                 'lemma': 'eka',\n",
      "                 'rel': 'root',\n",
      "                 'tag': 'NUM',\n",
      "                 'word': 'eka'},\n",
      "             10: {'address': 10,\n",
      "                  'ctag': 'AUX',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': 'Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act',\n",
      "                  'head': 9,\n",
      "                  'lemma': 'hE',\n",
      "                  'rel': 'cop',\n",
      "                  'tag': 'AUX',\n",
      "                  'word': 'hE'},\n",
      "             11: {'address': 11,\n",
      "                  'ctag': 'PUNCT',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 9,\n",
      "                  'lemma': '.',\n",
      "                  'rel': 'punct',\n",
      "                  'tag': 'PUNCT',\n",
      "                  'word': '.'}})\n"
     ]
    }
   ],
   "source": [
    "# To take a look at how a Dependency Parse Tree Looks like\n",
    "print(f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model used for Classification\n",
      "Arc Standard Model with all features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8609221466364324\n",
      "Unlabelled attachment score:  0.7679516250944822\n"
     ]
    }
   ],
   "source": [
    "# SVM Model for Classification\n",
    "print('SVM Model used for Classification')\n",
    "# Arc Standard Model with all features\n",
    "print('Arc Standard Model with all features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model used for Classification\n",
      "Arc Eager Model with all features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8820861678004536\n",
      "Unlabelled attachment score:  0.7928949357520786\n"
     ]
    }
   ],
   "source": [
    "# SVM Model for Classification\n",
    "print('SVM Model used for Classification')\n",
    "# Arc Eager Model with all features\n",
    "print('Arc Eager Model with all features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            '''\n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            '''\n",
    "            model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            #model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model used for Classification\n",
      "Arc Standard Model with all features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8253968253968254\n",
      "Unlabelled attachment score:  0.7135298563869993\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Classification\n",
    "print('Logistic Regression Model used for Classification')\n",
    "# Arc Standard Model with all features\n",
    "print('Arc Standard Model with all features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model used for Classification\n",
      "Arc Eager Model with all features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8639455782312925\n",
      "Unlabelled attachment score:  0.7528344671201814\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Classification\n",
    "print('Logistic Regression Model used for Classification')\n",
    "# Arc Eager Model with all features\n",
    "print('Arc Eager Model with all features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            '''\n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            '''\n",
    "            #model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Model used for Classification\n",
      "Arc Standard Model with all features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8390022675736961\n",
      "Unlabelled attachment score:  0.7309145880574452\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "print('MLP Classifier Model used for Classification')\n",
    "# Arc Standard Model with all features\n",
    "print('Arc Standard Model with all features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Model used for Classification\n",
      "Arc Eager Model with all features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8654572940287226\n",
      "Unlabelled attachment score:  0.7603930461073318\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "print('MLP Classifier Model used for Classification')\n",
    "# Arc Eager Model with all features\n",
    "print('Arc Eager Model with all features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Class without Morphological Features for Parser without Morphological Features\n",
    "# Removed Morphological Features by commenting out the concerned part in the Extract Features Function\n",
    "class Configuration(object):\n",
    "    \"\"\"\n",
    "    Class for holding configuration which is the partial analysis of the input sentence.\n",
    "    The transition based parser aims at finding set of operators that transfer the initial\n",
    "    configuration to the terminal configuration.\n",
    "\n",
    "    The configuration includes:\n",
    "        - Stack: for storing partially proceeded words\n",
    "        - Buffer: for storing remaining input words\n",
    "        - Set of arcs: for storing partially built dependency tree\n",
    "\n",
    "    This class also provides a method to represent a configuration as list of features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dep_graph):\n",
    "        \"\"\"\n",
    "        :param dep_graph: the representation of an input in the form of dependency graph.\n",
    "        :type dep_graph: DependencyGraph where the dependencies are not specified.\n",
    "        \"\"\"\n",
    "        # dep_graph.nodes contain list of token for a sentence\n",
    "        self.stack = [0]  # The root element\n",
    "        self.buffer = list(range(1, len(dep_graph.nodes)))  # The rest is in the buffer\n",
    "        self.arcs = []  # empty set of arc\n",
    "        self._tokens = dep_graph.nodes\n",
    "        self._max_address = len(self.buffer)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Stack : ' + \\\n",
    "            str(self.stack) + '  Buffer : ' + str(self.buffer) + '   Arcs : ' + str(self.arcs)\n",
    "\n",
    "    def _check_informative(self, feat, flag=False):\n",
    "        \"\"\"\n",
    "        Check whether a feature is informative\n",
    "        The flag control whether \"_\" is informative or not\n",
    "        \"\"\"\n",
    "        if feat is None:\n",
    "            return False\n",
    "        if feat == '':\n",
    "            return False\n",
    "        if flag is False:\n",
    "            if feat == '_':\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def extract_features(self):\n",
    "        \"\"\"\n",
    "        Extract the set of features for the current configuration. Implement standard features as describe in\n",
    "        Table 3.2 (page 31) in Dependency Parsing book by Sandra Kubler, Ryan McDonal, Joakim Nivre.\n",
    "        Please note that these features are very basic.\n",
    "        :return: list(str)\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # Todo : can come up with more complicated features set for better\n",
    "        # performance.\n",
    "        if len(self.stack) > 0:\n",
    "            # Stack 0\n",
    "            stack_idx0 = self.stack[len(self.stack) - 1]\n",
    "            token = self._tokens[stack_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('STK_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('STK_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('STK_0_POS_' + token['tag'])\n",
    "            '''\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('STK_0_FEATS_' + feat)\n",
    "            '''\n",
    "            # Stack 1\n",
    "            if len(self.stack) > 1:\n",
    "                stack_idx1 = self.stack[len(self.stack) - 2]\n",
    "                token = self._tokens[stack_idx1]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('STK_1_POS_' + token['tag'])\n",
    "\n",
    "            # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == stack_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('STK_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('STK_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        # Check Buffered 0\n",
    "        if len(self.buffer) > 0:\n",
    "            # Buffer 0\n",
    "            buffer_idx0 = self.buffer[0]\n",
    "            token = self._tokens[buffer_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('BUF_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('BUF_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('BUF_0_POS_' + token['tag'])\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('BUF_0_FEATS_' + feat)\n",
    "            # Buffer 1\n",
    "            if len(self.buffer) > 1:\n",
    "                buffer_idx1 = self.buffer[1]\n",
    "                token = self._tokens[buffer_idx1]\n",
    "                if self._check_informative(token['word'], True):\n",
    "                    result.append('BUF_1_FORM_' + token['word'])\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_1_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 2:\n",
    "                buffer_idx2 = self.buffer[2]\n",
    "                token = self._tokens[buffer_idx2]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_2_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 3:\n",
    "                buffer_idx3 = self.buffer[3]\n",
    "                token = self._tokens[buffer_idx3]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_3_POS_' + token['tag'])\n",
    "                    # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == buffer_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('BUF_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('BUF_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            \n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            \n",
    "            #model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            #model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model used for Classification\n",
      "Arc Standard Model without Morphological features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8450491307634165\n",
      "Unlabelled attachment score:  0.7573696145124716\n"
     ]
    }
   ],
   "source": [
    "# SVM Model for Classification\n",
    "print('SVM Model used for Classification')\n",
    "# Arc Standard Model without Morphological features\n",
    "print('Arc Standard Model without Morphological features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model used for Classification\n",
      "Arc Eager Model without Morphological features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8631897203325775\n",
      "Unlabelled attachment score:  0.7671957671957672\n"
     ]
    }
   ],
   "source": [
    "# SVM Model for Classification\n",
    "print('SVM Model used for Classification')\n",
    "# Arc Eager Model without Morphological features\n",
    "print('Arc Eager Model without Morphological features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            '''\n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            '''\n",
    "            model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            #model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model used for Classification\n",
      "Arc Standard Model without Morphological features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8148148148148148\n",
      "Unlabelled attachment score:  0.7059712773998488\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Classification\n",
    "print('Logistic Regression Model used for Classification')\n",
    "# Arc Standard Model without Morphological features\n",
    "print('Arc Standard Model without Morphological features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model used for Classification\n",
      "Arc Eager Model without Morphological features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8518518518518519\n",
      "Unlabelled attachment score:  0.7362055933484505\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Classification\n",
    "print('Logistic Regression Model used for Classification')\n",
    "# Arc Eager Model without Morphological features\n",
    "print('Arc Eager Model without Morphological features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            ''''\n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            '''\n",
    "            #model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Model used for Classification\n",
      "Arc Standard Model without Morphological features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8208616780045351\n",
      "Unlabelled attachment score:  0.7195767195767195\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "print('MLP Classifier Model used for Classification')\n",
    "# Arc Standard Model without Morphological features\n",
    "print('Arc Standard Model without Morphological features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Model used for Classification\n",
      "Arc Eager Model without Morphological features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8684807256235828\n",
      "Unlabelled attachment score:  0.763416477702192\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "print('MLP Classifier Model used for Classification')\n",
    "# Arc Eager Model without Morphological features\n",
    "print('Arc Eager Model without Morphological features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Class with only Word Feature\n",
    "class Configuration(object):\n",
    "    \"\"\"\n",
    "    Class for holding configuration which is the partial analysis of the input sentence.\n",
    "    The transition based parser aims at finding set of operators that transfer the initial\n",
    "    configuration to the terminal configuration.\n",
    "\n",
    "    The configuration includes:\n",
    "        - Stack: for storing partially proceeded words\n",
    "        - Buffer: for storing remaining input words\n",
    "        - Set of arcs: for storing partially built dependency tree\n",
    "\n",
    "    This class also provides a method to represent a configuration as list of features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dep_graph):\n",
    "        \"\"\"\n",
    "        :param dep_graph: the representation of an input in the form of dependency graph.\n",
    "        :type dep_graph: DependencyGraph where the dependencies are not specified.\n",
    "        \"\"\"\n",
    "        # dep_graph.nodes contain list of token for a sentence\n",
    "        self.stack = [0]  # The root element\n",
    "        self.buffer = list(range(1, len(dep_graph.nodes)))  # The rest is in the buffer\n",
    "        self.arcs = []  # empty set of arc\n",
    "        self._tokens = dep_graph.nodes\n",
    "        self._max_address = len(self.buffer)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Stack : ' + \\\n",
    "            str(self.stack) + '  Buffer : ' + str(self.buffer) + '   Arcs : ' + str(self.arcs)\n",
    "\n",
    "    def _check_informative(self, feat, flag=False):\n",
    "        \"\"\"\n",
    "        Check whether a feature is informative\n",
    "        The flag control whether \"_\" is informative or not\n",
    "        \"\"\"\n",
    "        if feat is None:\n",
    "            return False\n",
    "        if feat == '':\n",
    "            return False\n",
    "        if flag is False:\n",
    "            if feat == '_':\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def extract_features(self):\n",
    "        \"\"\"\n",
    "        Extract the set of features for the current configuration. Implement standard features as describe in\n",
    "        Table 3.2 (page 31) in Dependency Parsing book by Sandra Kubler, Ryan McDonal, Joakim Nivre.\n",
    "        Please note that these features are very basic.\n",
    "        :return: list(str)\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # Todo : can come up with more complicated features set for better\n",
    "        # performance.\n",
    "        if len(self.stack) > 0:\n",
    "            # Stack 0\n",
    "            stack_idx0 = self.stack[len(self.stack) - 1]\n",
    "            token = self._tokens[stack_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('STK_0_FORM_' + token['word'])\n",
    "            '''\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('STK_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('STK_0_POS_' + token['tag'])\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('STK_0_FEATS_' + feat)\n",
    "            '''\n",
    "            # Stack 1\n",
    "            if len(self.stack) > 1:\n",
    "                stack_idx1 = self.stack[len(self.stack) - 2]\n",
    "                token = self._tokens[stack_idx1]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('STK_1_POS_' + token['tag'])\n",
    "\n",
    "            # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == stack_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('STK_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('STK_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        # Check Buffered 0\n",
    "        if len(self.buffer) > 0:\n",
    "            # Buffer 0\n",
    "            buffer_idx0 = self.buffer[0]\n",
    "            token = self._tokens[buffer_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('BUF_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('BUF_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('BUF_0_POS_' + token['tag'])\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('BUF_0_FEATS_' + feat)\n",
    "            # Buffer 1\n",
    "            if len(self.buffer) > 1:\n",
    "                buffer_idx1 = self.buffer[1]\n",
    "                token = self._tokens[buffer_idx1]\n",
    "                if self._check_informative(token['word'], True):\n",
    "                    result.append('BUF_1_FORM_' + token['word'])\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_1_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 2:\n",
    "                buffer_idx2 = self.buffer[2]\n",
    "                token = self._tokens[buffer_idx2]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_2_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 3:\n",
    "                buffer_idx3 = self.buffer[3]\n",
    "                token = self._tokens[buffer_idx3]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_3_POS_' + token['tag'])\n",
    "                    # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == buffer_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('BUF_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('BUF_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            \n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            \n",
    "            #model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            #model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model used for Classification\n",
      "Arc Standard Model with only Word feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8155706727135299\n",
      "Unlabelled attachment score:  0.6893424036281179\n"
     ]
    }
   ],
   "source": [
    "# SVM Model for Classification\n",
    "print('SVM Model used for Classification')\n",
    "# Arc Standard Model with only Word feature\n",
    "print('Arc Standard Model with only Word feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model used for Classification\n",
      "Arc Eager Model with only Word feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8344671201814059\n",
      "Unlabelled attachment score:  0.7067271352985639\n"
     ]
    }
   ],
   "source": [
    "# SVM Model for Classification\n",
    "print('SVM Model used for Classification')\n",
    "# Arc Eager Model with only Word feature\n",
    "print('Arc Eager Model with only Word feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            '''\n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            '''\n",
    "            model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            #model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model used for Classification\n",
      "Arc Standard Model with only Word feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.7732426303854876\n",
      "Unlabelled attachment score:  0.6402116402116402\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Classification\n",
    "print('Logistic Regression Model used for Classification')\n",
    "# Arc Standard Model with only Word feature\n",
    "print('Arc Standard Model with only Word feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model used for Classification\n",
      "Arc Eager Model with only Word feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8102796674225246\n",
      "Unlabelled attachment score:  0.6764928193499622\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Classification\n",
    "print('Logistic Regression Model used for Classification')\n",
    "# Arc Eager Model with only Word feature\n",
    "print('Arc Eager Model with only Word feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            '''\n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            '''\n",
    "            #model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Model used for Classification\n",
      "Arc Standard Model with only Word feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8049886621315193\n",
      "Unlabelled attachment score:  0.6704459561602418\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "print('MLP Classifier Model used for Classification')\n",
    "# Arc Standard Model with only Word feature\n",
    "print('Arc Standard Model with only Word feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Model used for Classification\n",
      "Arc Eager Model with only Word feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8284202569916855\n",
      "Unlabelled attachment score:  0.6810279667422524\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "print('MLP Classifier Model used for Classification')\n",
    "# Arc Eager Model with only Word feature\n",
    "print('Arc Eager Model with only Word feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Class with only Lemma Feature\n",
    "class Configuration(object):\n",
    "    \"\"\"\n",
    "    Class for holding configuration which is the partial analysis of the input sentence.\n",
    "    The transition based parser aims at finding set of operators that transfer the initial\n",
    "    configuration to the terminal configuration.\n",
    "\n",
    "    The configuration includes:\n",
    "        - Stack: for storing partially proceeded words\n",
    "        - Buffer: for storing remaining input words\n",
    "        - Set of arcs: for storing partially built dependency tree\n",
    "\n",
    "    This class also provides a method to represent a configuration as list of features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dep_graph):\n",
    "        \"\"\"\n",
    "        :param dep_graph: the representation of an input in the form of dependency graph.\n",
    "        :type dep_graph: DependencyGraph where the dependencies are not specified.\n",
    "        \"\"\"\n",
    "        # dep_graph.nodes contain list of token for a sentence\n",
    "        self.stack = [0]  # The root element\n",
    "        self.buffer = list(range(1, len(dep_graph.nodes)))  # The rest is in the buffer\n",
    "        self.arcs = []  # empty set of arc\n",
    "        self._tokens = dep_graph.nodes\n",
    "        self._max_address = len(self.buffer)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Stack : ' + \\\n",
    "            str(self.stack) + '  Buffer : ' + str(self.buffer) + '   Arcs : ' + str(self.arcs)\n",
    "\n",
    "    def _check_informative(self, feat, flag=False):\n",
    "        \"\"\"\n",
    "        Check whether a feature is informative\n",
    "        The flag control whether \"_\" is informative or not\n",
    "        \"\"\"\n",
    "        if feat is None:\n",
    "            return False\n",
    "        if feat == '':\n",
    "            return False\n",
    "        if flag is False:\n",
    "            if feat == '_':\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def extract_features(self):\n",
    "        \"\"\"\n",
    "        Extract the set of features for the current configuration. Implement standard features as describe in\n",
    "        Table 3.2 (page 31) in Dependency Parsing book by Sandra Kubler, Ryan McDonal, Joakim Nivre.\n",
    "        Please note that these features are very basic.\n",
    "        :return: list(str)\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # Todo : can come up with more complicated features set for better\n",
    "        # performance.\n",
    "        if len(self.stack) > 0:\n",
    "            # Stack 0\n",
    "            stack_idx0 = self.stack[len(self.stack) - 1]\n",
    "            token = self._tokens[stack_idx0]\n",
    "            '''\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('STK_0_FORM_' + token['word'])\n",
    "            '''\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('STK_0_LEMMA_' + token['lemma'])\n",
    "            '''\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('STK_0_POS_' + token['tag'])\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('STK_0_FEATS_' + feat)\n",
    "            '''\n",
    "            # Stack 1\n",
    "            if len(self.stack) > 1:\n",
    "                stack_idx1 = self.stack[len(self.stack) - 2]\n",
    "                token = self._tokens[stack_idx1]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('STK_1_POS_' + token['tag'])\n",
    "\n",
    "            # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == stack_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('STK_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('STK_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        # Check Buffered 0\n",
    "        if len(self.buffer) > 0:\n",
    "            # Buffer 0\n",
    "            buffer_idx0 = self.buffer[0]\n",
    "            token = self._tokens[buffer_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('BUF_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('BUF_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('BUF_0_POS_' + token['tag'])\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('BUF_0_FEATS_' + feat)\n",
    "            # Buffer 1\n",
    "            if len(self.buffer) > 1:\n",
    "                buffer_idx1 = self.buffer[1]\n",
    "                token = self._tokens[buffer_idx1]\n",
    "                if self._check_informative(token['word'], True):\n",
    "                    result.append('BUF_1_FORM_' + token['word'])\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_1_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 2:\n",
    "                buffer_idx2 = self.buffer[2]\n",
    "                token = self._tokens[buffer_idx2]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_2_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 3:\n",
    "                buffer_idx3 = self.buffer[3]\n",
    "                token = self._tokens[buffer_idx3]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_3_POS_' + token['tag'])\n",
    "                    # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == buffer_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('BUF_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('BUF_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            \n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            \n",
    "            #model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            #model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model used for Classification\n",
      "Arc Standard Model with only Lemma feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8148148148148148\n",
      "Unlabelled attachment score:  0.6848072562358276\n"
     ]
    }
   ],
   "source": [
    "# SVM Model for Classification\n",
    "print('SVM Model used for Classification')\n",
    "# Arc Standard Model with only Lemma feature\n",
    "print('Arc Standard Model with only Lemma feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model used for Classification\n",
      "Arc Eager Model with only Lemma feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.81859410430839\n",
      "Unlabelled attachment score:  0.690098261526833\n"
     ]
    }
   ],
   "source": [
    "# SVM Model for Classification\n",
    "print('SVM Model used for Classification')\n",
    "# Arc Eager Model with only Lemma Feature\n",
    "print('Arc Eager Model with only Lemma feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            '''\n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            '''\n",
    "            model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            #model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model used for Classification\n",
      "Arc Standard Model with only Lemma feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.7717309145880574\n",
      "Unlabelled attachment score:  0.6349206349206349\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Classification\n",
    "print('Logistic Regression Model used for Classification')\n",
    "# Arc Standard Model with only Lemma feature\n",
    "print('Arc Standard Model with only Lemma feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model used for Classification\n",
      "Arc Eager Model with only Lemma feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8034769463340892\n",
      "Unlabelled attachment score:  0.6681783824640968\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Classification\n",
    "print('Logistic Regression Model used for Classification')\n",
    "# Arc Eager Model with only Lemma feature\n",
    "print('Arc Eager Model with only Lemma feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            '''\n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            '''\n",
    "            #model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Model used for Classification\n",
      "Arc Standard Model with only Lemma feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.7959183673469388\n",
      "Unlabelled attachment score:  0.6523053665910808\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "print('MLP Classifier Model used for Classification')\n",
    "# Arc Standard Model with only Lemma feature\n",
    "print('Arc Standard Model with only Lemma feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Model used for Classification\n",
      "Arc Eager Model with only Lemma feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8276643990929705\n",
      "Unlabelled attachment score:  0.6749811035525322\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "print('MLP Classifier Model used for Classification')\n",
    "# Arc Eager Model with only Lemma feature\n",
    "print('Arc Eager Model with only Lemma feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Class with only Tag Feature\n",
    "class Configuration(object):\n",
    "    \"\"\"\n",
    "    Class for holding configuration which is the partial analysis of the input sentence.\n",
    "    The transition based parser aims at finding set of operators that transfer the initial\n",
    "    configuration to the terminal configuration.\n",
    "\n",
    "    The configuration includes:\n",
    "        - Stack: for storing partially proceeded words\n",
    "        - Buffer: for storing remaining input words\n",
    "        - Set of arcs: for storing partially built dependency tree\n",
    "\n",
    "    This class also provides a method to represent a configuration as list of features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dep_graph):\n",
    "        \"\"\"\n",
    "        :param dep_graph: the representation of an input in the form of dependency graph.\n",
    "        :type dep_graph: DependencyGraph where the dependencies are not specified.\n",
    "        \"\"\"\n",
    "        # dep_graph.nodes contain list of token for a sentence\n",
    "        self.stack = [0]  # The root element\n",
    "        self.buffer = list(range(1, len(dep_graph.nodes)))  # The rest is in the buffer\n",
    "        self.arcs = []  # empty set of arc\n",
    "        self._tokens = dep_graph.nodes\n",
    "        self._max_address = len(self.buffer)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Stack : ' + \\\n",
    "            str(self.stack) + '  Buffer : ' + str(self.buffer) + '   Arcs : ' + str(self.arcs)\n",
    "\n",
    "    def _check_informative(self, feat, flag=False):\n",
    "        \"\"\"\n",
    "        Check whether a feature is informative\n",
    "        The flag control whether \"_\" is informative or not\n",
    "        \"\"\"\n",
    "        if feat is None:\n",
    "            return False\n",
    "        if feat == '':\n",
    "            return False\n",
    "        if flag is False:\n",
    "            if feat == '_':\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def extract_features(self):\n",
    "        \"\"\"\n",
    "        Extract the set of features for the current configuration. Implement standard features as describe in\n",
    "        Table 3.2 (page 31) in Dependency Parsing book by Sandra Kubler, Ryan McDonal, Joakim Nivre.\n",
    "        Please note that these features are very basic.\n",
    "        :return: list(str)\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # Todo : can come up with more complicated features set for better\n",
    "        # performance.\n",
    "        if len(self.stack) > 0:\n",
    "            # Stack 0\n",
    "            stack_idx0 = self.stack[len(self.stack) - 1]\n",
    "            token = self._tokens[stack_idx0]\n",
    "            '''\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('STK_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('STK_0_LEMMA_' + token['lemma'])\n",
    "            '''\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('STK_0_POS_' + token['tag'])\n",
    "            '''\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('STK_0_FEATS_' + feat)\n",
    "            '''\n",
    "            # Stack 1\n",
    "            if len(self.stack) > 1:\n",
    "                stack_idx1 = self.stack[len(self.stack) - 2]\n",
    "                token = self._tokens[stack_idx1]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('STK_1_POS_' + token['tag'])\n",
    "\n",
    "            # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == stack_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('STK_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('STK_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        # Check Buffered 0\n",
    "        if len(self.buffer) > 0:\n",
    "            # Buffer 0\n",
    "            buffer_idx0 = self.buffer[0]\n",
    "            token = self._tokens[buffer_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('BUF_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('BUF_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('BUF_0_POS_' + token['tag'])\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('BUF_0_FEATS_' + feat)\n",
    "            # Buffer 1\n",
    "            if len(self.buffer) > 1:\n",
    "                buffer_idx1 = self.buffer[1]\n",
    "                token = self._tokens[buffer_idx1]\n",
    "                if self._check_informative(token['word'], True):\n",
    "                    result.append('BUF_1_FORM_' + token['word'])\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_1_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 2:\n",
    "                buffer_idx2 = self.buffer[2]\n",
    "                token = self._tokens[buffer_idx2]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_2_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 3:\n",
    "                buffer_idx3 = self.buffer[3]\n",
    "                token = self._tokens[buffer_idx3]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_3_POS_' + token['tag'])\n",
    "                    # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == buffer_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('BUF_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('BUF_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            \n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            \n",
    "            #model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            #model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model used for Classification\n",
      "Arc Standard Model with only Tag feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8435374149659864\n",
      "Unlabelled attachment score:  0.7505668934240363\n"
     ]
    }
   ],
   "source": [
    "# SVM Model for Classification\n",
    "print('SVM Model used for Classification')\n",
    "# Arc Standard Model with only Tag feature\n",
    "print('Arc Standard Model with only Tag feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model used for Classification\n",
      "Arc Eager Model with only Tag feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8321995464852607\n",
      "Unlabelled attachment score:  0.7331821617535903\n"
     ]
    }
   ],
   "source": [
    "# SVM Model for Classification\n",
    "print('SVM Model used for Classification')\n",
    "# Arc Eager Model with only Tag Feature\n",
    "print('Arc Eager Model with only Tag feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            '''\n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            '''\n",
    "            model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            #model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model used for Classification\n",
      "Arc Standard Model with only Tag feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8155706727135299\n",
      "Unlabelled attachment score:  0.7044595616024187\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Classification\n",
    "print('Logistic Regression Model used for Classification')\n",
    "# Arc Standard Model with only Tag feature\n",
    "print('Arc Standard Model with only Tag feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model used for Classification\n",
      "Arc Eager Model with only Tag feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8019652305366591\n",
      "Unlabelled attachment score:  0.6863189720332578\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Classification\n",
    "print('Logistic Regression Model used for Classification')\n",
    "# Arc Eager Model with only Tag feature\n",
    "print('Arc Eager Model with only Tag feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            '''\n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            '''\n",
    "            #model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Model used for Classification\n",
      "Arc Standard Model with only Tag feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8208616780045351\n",
      "Unlabelled attachment score:  0.7165532879818595\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "print('MLP Classifier Model used for Classification')\n",
    "# Arc Standard Model with only Tag feature\n",
    "print('Arc Standard Model with only Tag feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Model used for Classification\n",
      "Arc Eager Model with only Tag feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8193499622071051\n",
      "Unlabelled attachment score:  0.7112622826908541\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "print('MLP Classifier Model used for Classification')\n",
    "# Arc Eager Model with only Tag feature\n",
    "print('Arc Eager Model with only Tag feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Class with only Morphological Features\n",
    "class Configuration(object):\n",
    "    \"\"\"\n",
    "    Class for holding configuration which is the partial analysis of the input sentence.\n",
    "    The transition based parser aims at finding set of operators that transfer the initial\n",
    "    configuration to the terminal configuration.\n",
    "\n",
    "    The configuration includes:\n",
    "        - Stack: for storing partially proceeded words\n",
    "        - Buffer: for storing remaining input words\n",
    "        - Set of arcs: for storing partially built dependency tree\n",
    "\n",
    "    This class also provides a method to represent a configuration as list of features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dep_graph):\n",
    "        \"\"\"\n",
    "        :param dep_graph: the representation of an input in the form of dependency graph.\n",
    "        :type dep_graph: DependencyGraph where the dependencies are not specified.\n",
    "        \"\"\"\n",
    "        # dep_graph.nodes contain list of token for a sentence\n",
    "        self.stack = [0]  # The root element\n",
    "        self.buffer = list(range(1, len(dep_graph.nodes)))  # The rest is in the buffer\n",
    "        self.arcs = []  # empty set of arc\n",
    "        self._tokens = dep_graph.nodes\n",
    "        self._max_address = len(self.buffer)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Stack : ' + \\\n",
    "            str(self.stack) + '  Buffer : ' + str(self.buffer) + '   Arcs : ' + str(self.arcs)\n",
    "\n",
    "    def _check_informative(self, feat, flag=False):\n",
    "        \"\"\"\n",
    "        Check whether a feature is informative\n",
    "        The flag control whether \"_\" is informative or not\n",
    "        \"\"\"\n",
    "        if feat is None:\n",
    "            return False\n",
    "        if feat == '':\n",
    "            return False\n",
    "        if flag is False:\n",
    "            if feat == '_':\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def extract_features(self):\n",
    "        \"\"\"\n",
    "        Extract the set of features for the current configuration. Implement standard features as describe in\n",
    "        Table 3.2 (page 31) in Dependency Parsing book by Sandra Kubler, Ryan McDonal, Joakim Nivre.\n",
    "        Please note that these features are very basic.\n",
    "        :return: list(str)\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # Todo : can come up with more complicated features set for better\n",
    "        # performance.\n",
    "        if len(self.stack) > 0:\n",
    "            # Stack 0\n",
    "            stack_idx0 = self.stack[len(self.stack) - 1]\n",
    "            token = self._tokens[stack_idx0]\n",
    "            '''\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('STK_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('STK_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('STK_0_POS_' + token['tag'])\n",
    "            '''\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('STK_0_FEATS_' + feat)\n",
    "            # Stack 1\n",
    "            if len(self.stack) > 1:\n",
    "                stack_idx1 = self.stack[len(self.stack) - 2]\n",
    "                token = self._tokens[stack_idx1]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('STK_1_POS_' + token['tag'])\n",
    "\n",
    "            # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == stack_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('STK_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('STK_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        # Check Buffered 0\n",
    "        if len(self.buffer) > 0:\n",
    "            # Buffer 0\n",
    "            buffer_idx0 = self.buffer[0]\n",
    "            token = self._tokens[buffer_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('BUF_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('BUF_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('BUF_0_POS_' + token['tag'])\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('BUF_0_FEATS_' + feat)\n",
    "            # Buffer 1\n",
    "            if len(self.buffer) > 1:\n",
    "                buffer_idx1 = self.buffer[1]\n",
    "                token = self._tokens[buffer_idx1]\n",
    "                if self._check_informative(token['word'], True):\n",
    "                    result.append('BUF_1_FORM_' + token['word'])\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_1_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 2:\n",
    "                buffer_idx2 = self.buffer[2]\n",
    "                token = self._tokens[buffer_idx2]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_2_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 3:\n",
    "                buffer_idx3 = self.buffer[3]\n",
    "                token = self._tokens[buffer_idx3]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_3_POS_' + token['tag'])\n",
    "                    # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == buffer_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('BUF_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('BUF_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            \n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            \n",
    "            #model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            #model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model used for Classification\n",
      "Arc Standard Model with only Morphological features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8306878306878307\n",
      "Unlabelled attachment score:  0.7331821617535903\n"
     ]
    }
   ],
   "source": [
    "# SVM Model for Classification\n",
    "print('SVM Model used for Classification')\n",
    "# Arc Standard Model with only Morphological features\n",
    "print('Arc Standard Model with only Morphological features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model used for Classification\n",
      "Arc Eager Model with only Morphological features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8480725623582767\n",
      "Unlabelled attachment score:  0.7475434618291761\n"
     ]
    }
   ],
   "source": [
    "# SVM Model for Classification\n",
    "print('SVM Model used for Classification')\n",
    "# Arc Eager Model with only Morphological features\n",
    "print('Arc Eager Model with only Morphological features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            '''\n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            '''\n",
    "            model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            #model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model used for Classification\n",
      "Arc Standard Model with only Morphological features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.7792894935752078\n",
      "Unlabelled attachment score:  0.6689342403628118\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Classification\n",
    "print('Logistic Regression Model used for Classification')\n",
    "# Arc Standard Model with only Morphological features\n",
    "print('Arc Standard Model with only Morphological features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model used for Classification\n",
      "Arc Eager Model with only Morphological features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8049886621315193\n",
      "Unlabelled attachment score:  0.6938775510204082\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Classification\n",
    "print('Logistic Regression Model used for Classification')\n",
    "# Arc Eager Model with only Morphological features\n",
    "print('Arc Eager Model with only Morphological features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            '''\n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            '''\n",
    "            #model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Model used for Classification\n",
      "Arc Standard Model with only Morphological features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8193499622071051\n",
      "Unlabelled attachment score:  0.7135298563869993\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "print('MLP Classifier Model used for Classification')\n",
    "# Arc Standard Model with only Morphological features\n",
    "print('Arc Standard Model with only Morphological features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Model used for Classification\n",
      "Arc Eager Model with only Morphological features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.854875283446712\n",
      "Unlabelled attachment score:  0.72713529856387\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "print('MLP Classifier Model used for Classification')\n",
    "# Arc Eager Model with only Morphological features\n",
    "print('Arc Eager Model with only Morphological features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Class with only Word and Tag Features\n",
    "class Configuration(object):\n",
    "    \"\"\"\n",
    "    Class for holding configuration which is the partial analysis of the input sentence.\n",
    "    The transition based parser aims at finding set of operators that transfer the initial\n",
    "    configuration to the terminal configuration.\n",
    "\n",
    "    The configuration includes:\n",
    "        - Stack: for storing partially proceeded words\n",
    "        - Buffer: for storing remaining input words\n",
    "        - Set of arcs: for storing partially built dependency tree\n",
    "\n",
    "    This class also provides a method to represent a configuration as list of features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dep_graph):\n",
    "        \"\"\"\n",
    "        :param dep_graph: the representation of an input in the form of dependency graph.\n",
    "        :type dep_graph: DependencyGraph where the dependencies are not specified.\n",
    "        \"\"\"\n",
    "        # dep_graph.nodes contain list of token for a sentence\n",
    "        self.stack = [0]  # The root element\n",
    "        self.buffer = list(range(1, len(dep_graph.nodes)))  # The rest is in the buffer\n",
    "        self.arcs = []  # empty set of arc\n",
    "        self._tokens = dep_graph.nodes\n",
    "        self._max_address = len(self.buffer)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Stack : ' + \\\n",
    "            str(self.stack) + '  Buffer : ' + str(self.buffer) + '   Arcs : ' + str(self.arcs)\n",
    "\n",
    "    def _check_informative(self, feat, flag=False):\n",
    "        \"\"\"\n",
    "        Check whether a feature is informative\n",
    "        The flag control whether \"_\" is informative or not\n",
    "        \"\"\"\n",
    "        if feat is None:\n",
    "            return False\n",
    "        if feat == '':\n",
    "            return False\n",
    "        if flag is False:\n",
    "            if feat == '_':\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def extract_features(self):\n",
    "        \"\"\"\n",
    "        Extract the set of features for the current configuration. Implement standard features as describe in\n",
    "        Table 3.2 (page 31) in Dependency Parsing book by Sandra Kubler, Ryan McDonal, Joakim Nivre.\n",
    "        Please note that these features are very basic.\n",
    "        :return: list(str)\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # Todo : can come up with more complicated features set for better\n",
    "        # performance.\n",
    "        if len(self.stack) > 0:\n",
    "            # Stack 0\n",
    "            stack_idx0 = self.stack[len(self.stack) - 1]\n",
    "            token = self._tokens[stack_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('STK_0_FORM_' + token['word'])\n",
    "            #if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "            #    result.append('STK_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('STK_0_POS_' + token['tag'])\n",
    "            '''\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('STK_0_FEATS_' + feat)\n",
    "            '''\n",
    "            # Stack 1\n",
    "            if len(self.stack) > 1:\n",
    "                stack_idx1 = self.stack[len(self.stack) - 2]\n",
    "                token = self._tokens[stack_idx1]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('STK_1_POS_' + token['tag'])\n",
    "\n",
    "            # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == stack_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('STK_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('STK_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        # Check Buffered 0\n",
    "        if len(self.buffer) > 0:\n",
    "            # Buffer 0\n",
    "            buffer_idx0 = self.buffer[0]\n",
    "            token = self._tokens[buffer_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('BUF_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('BUF_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('BUF_0_POS_' + token['tag'])\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('BUF_0_FEATS_' + feat)\n",
    "            # Buffer 1\n",
    "            if len(self.buffer) > 1:\n",
    "                buffer_idx1 = self.buffer[1]\n",
    "                token = self._tokens[buffer_idx1]\n",
    "                if self._check_informative(token['word'], True):\n",
    "                    result.append('BUF_1_FORM_' + token['word'])\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_1_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 2:\n",
    "                buffer_idx2 = self.buffer[2]\n",
    "                token = self._tokens[buffer_idx2]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_2_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 3:\n",
    "                buffer_idx3 = self.buffer[3]\n",
    "                token = self._tokens[buffer_idx3]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_3_POS_' + token['tag'])\n",
    "                    # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == buffer_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('BUF_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('BUF_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            \n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            \n",
    "            #model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            #model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model used for Classification\n",
      "Arc Standard Model with only Word and Tag features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8488284202569917\n",
      "Unlabelled attachment score:  0.7596371882086168\n"
     ]
    }
   ],
   "source": [
    "# SVM Model for Classification\n",
    "print('SVM Model used for Classification')\n",
    "# Arc Standard Model with only Word and Tag features\n",
    "print('Arc Standard Model with only Word and Tag features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model used for Classification\n",
      "Arc Eager Model with only Word and Tag features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8609221466364324\n",
      "Unlabelled attachment score:  0.764172335600907\n"
     ]
    }
   ],
   "source": [
    "# SVM Model for Classification\n",
    "print('SVM Model used for Classification')\n",
    "# Arc Eager Model with only Word and Tag features\n",
    "print('Arc Eager Model with only Word and Tag features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            '''\n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            '''\n",
    "            model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            #model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model used for Classification\n",
      "Arc Standard Model with only Word and Tag features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8148148148148148\n",
      "Unlabelled attachment score:  0.7059712773998488\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Classification\n",
    "print('Logistic Regression Model used for Classification')\n",
    "# Arc Standard Model with only Word and Tag features\n",
    "print('Arc Standard Model with only Word and Tag features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model used for Classification\n",
      "Arc Eager Model with only Word and Tag features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8480725623582767\n",
      "Unlabelled attachment score:  0.7369614512471655\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Classification\n",
    "print('Logistic Regression Model used for Classification')\n",
    "# Arc Eager Model with only Word and Tag features\n",
    "print('Arc Eager Model with only Word and Tag features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            '''\n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            '''\n",
    "            #model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Model used for Classification\n",
      "Arc Standard Model with only Word and Tag features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8155706727135299\n",
      "Unlabelled attachment score:  0.7233560090702947\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "print('MLP Classifier Model used for Classification')\n",
    "# Arc Standard Model with only Word and Tag features\n",
    "print('Arc Standard Model with only Word and Tag features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Model used for Classification\n",
      "Arc Eager Model with only Word and Tag features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8359788359788359\n",
      "Unlabelled attachment score:  0.7316704459561603\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "print('MLP Classifier Model used for Classification')\n",
    "# Arc Eager Model with only Word and Tag features\n",
    "print('Arc Eager Model with only Word and Tag features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Class with only Word and Morphological Features\n",
    "class Configuration(object):\n",
    "    \"\"\"\n",
    "    Class for holding configuration which is the partial analysis of the input sentence.\n",
    "    The transition based parser aims at finding set of operators that transfer the initial\n",
    "    configuration to the terminal configuration.\n",
    "\n",
    "    The configuration includes:\n",
    "        - Stack: for storing partially proceeded words\n",
    "        - Buffer: for storing remaining input words\n",
    "        - Set of arcs: for storing partially built dependency tree\n",
    "\n",
    "    This class also provides a method to represent a configuration as list of features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dep_graph):\n",
    "        \"\"\"\n",
    "        :param dep_graph: the representation of an input in the form of dependency graph.\n",
    "        :type dep_graph: DependencyGraph where the dependencies are not specified.\n",
    "        \"\"\"\n",
    "        # dep_graph.nodes contain list of token for a sentence\n",
    "        self.stack = [0]  # The root element\n",
    "        self.buffer = list(range(1, len(dep_graph.nodes)))  # The rest is in the buffer\n",
    "        self.arcs = []  # empty set of arc\n",
    "        self._tokens = dep_graph.nodes\n",
    "        self._max_address = len(self.buffer)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Stack : ' + \\\n",
    "            str(self.stack) + '  Buffer : ' + str(self.buffer) + '   Arcs : ' + str(self.arcs)\n",
    "\n",
    "    def _check_informative(self, feat, flag=False):\n",
    "        \"\"\"\n",
    "        Check whether a feature is informative\n",
    "        The flag control whether \"_\" is informative or not\n",
    "        \"\"\"\n",
    "        if feat is None:\n",
    "            return False\n",
    "        if feat == '':\n",
    "            return False\n",
    "        if flag is False:\n",
    "            if feat == '_':\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def extract_features(self):\n",
    "        \"\"\"\n",
    "        Extract the set of features for the current configuration. Implement standard features as describe in\n",
    "        Table 3.2 (page 31) in Dependency Parsing book by Sandra Kubler, Ryan McDonal, Joakim Nivre.\n",
    "        Please note that these features are very basic.\n",
    "        :return: list(str)\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # Todo : can come up with more complicated features set for better\n",
    "        # performance.\n",
    "        if len(self.stack) > 0:\n",
    "            # Stack 0\n",
    "            stack_idx0 = self.stack[len(self.stack) - 1]\n",
    "            token = self._tokens[stack_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('STK_0_FORM_' + token['word'])\n",
    "            #if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "            #    result.append('STK_0_LEMMA_' + token['lemma'])\n",
    "            #if self._check_informative(token['tag']):\n",
    "            #    result.append('STK_0_POS_' + token['tag'])\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('STK_0_FEATS_' + feat)\n",
    "            # Stack 1\n",
    "            if len(self.stack) > 1:\n",
    "                stack_idx1 = self.stack[len(self.stack) - 2]\n",
    "                token = self._tokens[stack_idx1]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('STK_1_POS_' + token['tag'])\n",
    "\n",
    "            # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == stack_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('STK_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('STK_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        # Check Buffered 0\n",
    "        if len(self.buffer) > 0:\n",
    "            # Buffer 0\n",
    "            buffer_idx0 = self.buffer[0]\n",
    "            token = self._tokens[buffer_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('BUF_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('BUF_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('BUF_0_POS_' + token['tag'])\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('BUF_0_FEATS_' + feat)\n",
    "            # Buffer 1\n",
    "            if len(self.buffer) > 1:\n",
    "                buffer_idx1 = self.buffer[1]\n",
    "                token = self._tokens[buffer_idx1]\n",
    "                if self._check_informative(token['word'], True):\n",
    "                    result.append('BUF_1_FORM_' + token['word'])\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_1_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 2:\n",
    "                buffer_idx2 = self.buffer[2]\n",
    "                token = self._tokens[buffer_idx2]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_2_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 3:\n",
    "                buffer_idx3 = self.buffer[3]\n",
    "                token = self._tokens[buffer_idx3]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_3_POS_' + token['tag'])\n",
    "                    # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == buffer_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('BUF_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('BUF_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            \n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            \n",
    "            #model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            #model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model used for Classification\n",
      "Arc Standard Model with only Word and Morphological features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8359788359788359\n",
      "Unlabelled attachment score:  0.7407407407407407\n"
     ]
    }
   ],
   "source": [
    "# SVM Model for Classification\n",
    "print('SVM Model used for Classification')\n",
    "# Arc Standard Model with only Word and Morphological features\n",
    "print('Arc Standard Model with only Word and Morphological features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model used for Classification\n",
      "Arc Eager Model with only Word and Morphological features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8662131519274376\n",
      "Unlabelled attachment score:  0.764172335600907\n"
     ]
    }
   ],
   "source": [
    "# SVM Model for Classification\n",
    "print('SVM Model used for Classification')\n",
    "# Arc Eager Model with only Word and Morphological features\n",
    "print('Arc Eager Model with only Word and Morphological features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            '''\n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            '''\n",
    "            model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            #model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model used for Classification\n",
      "Arc Standard Model with only Word and Morphological features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.798941798941799\n",
      "Unlabelled attachment score:  0.691609977324263\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Classification\n",
    "print('Logistic Regression Model used for Classification')\n",
    "# Arc Standard Model with only Word and Morphological features\n",
    "print('Arc Standard Model with only Word and Morphological features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model used for Classification\n",
      "Arc Eager Model with only Word and Morphological features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8412698412698413\n",
      "Unlabelled attachment score:  0.7324263038548753\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Classification\n",
    "print('Logistic Regression Model used for Classification')\n",
    "# Arc Eager Model with only Word and Morphological features\n",
    "print('Arc Eager Model with only Word and Morphological features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            '''\n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            '''\n",
    "            #model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Model used for Classification\n",
      "Arc Standard Model with only Word and Morphological features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8374905517762661\n",
      "Unlabelled attachment score:  0.7286470143613001\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "print('MLP Classifier Model used for Classification')\n",
    "# Arc Standard Model with only Word and Morphological features\n",
    "print('Arc Standard Model with only Word and Morphological features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Model used for Classification\n",
      "Arc Eager Model with only Word and Morphological features\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8798185941043084\n",
      "Unlabelled attachment score:  0.7619047619047619\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "print('MLP Classifier Model used for Classification')\n",
    "# Arc Eager Model with only Word and Morphological features\n",
    "print('Arc Eager Model with only Word and Morphological features')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With all the original features plus a new feature 'REL' which denotes the \n",
    "# Universal dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one\n",
    "class Configuration(object):\n",
    "    \"\"\"\n",
    "    Class for holding configuration which is the partial analysis of the input sentence.\n",
    "    The transition based parser aims at finding set of operators that transfer the initial\n",
    "    configuration to the terminal configuration.\n",
    "\n",
    "    The configuration includes:\n",
    "        - Stack: for storing partially proceeded words\n",
    "        - Buffer: for storing remaining input words\n",
    "        - Set of arcs: for storing partially built dependency tree\n",
    "\n",
    "    This class also provides a method to represent a configuration as list of features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dep_graph):\n",
    "        \"\"\"\n",
    "        :param dep_graph: the representation of an input in the form of dependency graph.\n",
    "        :type dep_graph: DependencyGraph where the dependencies are not specified.\n",
    "        \"\"\"\n",
    "        # dep_graph.nodes contain list of token for a sentence\n",
    "        self.stack = [0]  # The root element\n",
    "        self.buffer = list(range(1, len(dep_graph.nodes)))  # The rest is in the buffer\n",
    "        self.arcs = []  # empty set of arc\n",
    "        self._tokens = dep_graph.nodes\n",
    "        self._max_address = len(self.buffer)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Stack : ' + \\\n",
    "            str(self.stack) + '  Buffer : ' + str(self.buffer) + '   Arcs : ' + str(self.arcs)\n",
    "\n",
    "    def _check_informative(self, feat, flag=False):\n",
    "        \"\"\"\n",
    "        Check whether a feature is informative\n",
    "        The flag control whether \"_\" is informative or not\n",
    "        \"\"\"\n",
    "        if feat is None:\n",
    "            return False\n",
    "        if feat == '':\n",
    "            return False\n",
    "        if flag is False:\n",
    "            if feat == '_':\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def extract_features(self):\n",
    "        \"\"\"\n",
    "        Extract the set of features for the current configuration. Implement standard features as describe in\n",
    "        Table 3.2 (page 31) in Dependency Parsing book by Sandra Kubler, Ryan McDonal, Joakim Nivre.\n",
    "        Please note that these features are very basic.\n",
    "        :return: list(str)\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # Todo : can come up with more complicated features set for better\n",
    "        # performance.\n",
    "        if len(self.stack) > 0:\n",
    "            # Stack 0\n",
    "            stack_idx0 = self.stack[len(self.stack) - 1]\n",
    "            token = self._tokens[stack_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('STK_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('STK_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('STK_0_POS_' + token['tag'])\n",
    "            if self._check_informative(token['rel']):\n",
    "                result.append('STK_0_REL_' + token['rel'])\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('STK_0_FEATS_' + feat)\n",
    "            # Stack 1\n",
    "            if len(self.stack) > 1:\n",
    "                stack_idx1 = self.stack[len(self.stack) - 2]\n",
    "                token = self._tokens[stack_idx1]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('STK_1_POS_' + token['tag'])\n",
    "\n",
    "            # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == stack_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('STK_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('STK_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        # Check Buffered 0\n",
    "        if len(self.buffer) > 0:\n",
    "            # Buffer 0\n",
    "            buffer_idx0 = self.buffer[0]\n",
    "            token = self._tokens[buffer_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('BUF_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('BUF_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('BUF_0_POS_' + token['tag'])\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('BUF_0_FEATS_' + feat)\n",
    "            # Buffer 1\n",
    "            if len(self.buffer) > 1:\n",
    "                buffer_idx1 = self.buffer[1]\n",
    "                token = self._tokens[buffer_idx1]\n",
    "                if self._check_informative(token['word'], True):\n",
    "                    result.append('BUF_1_FORM_' + token['word'])\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_1_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 2:\n",
    "                buffer_idx2 = self.buffer[2]\n",
    "                token = self._tokens[buffer_idx2]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_2_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 3:\n",
    "                buffer_idx3 = self.buffer[3]\n",
    "                token = self._tokens[buffer_idx3]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_3_POS_' + token['tag'])\n",
    "                    # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == buffer_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('BUF_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('BUF_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            \n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            \n",
    "            #model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            #model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model used for Classification\n",
      "Arc Standard Model with all features plus the new feature REL\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8843537414965986\n",
      "Unlabelled attachment score:  0.8291761148904006\n"
     ]
    }
   ],
   "source": [
    "# SVM Model for Classification\n",
    "print('SVM Model used for Classification')\n",
    "# Arc Standard Model with all features plus the new feature REL\n",
    "print('Arc Standard Model with all features plus the new feature REL')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model used for Classification\n",
      "Arc Eager Model with all features plus the new feature REL\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8964474678760394\n",
      "Unlabelled attachment score:  0.8420256991685563\n"
     ]
    }
   ],
   "source": [
    "# SVM Model for Classification\n",
    "print('SVM Model used for Classification')\n",
    "# Arc Eager Model with all features plus the new feature REL\n",
    "print('Arc Eager Model with all features plus the new feature REL')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            '''\n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            '''\n",
    "            model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            #model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model used for Classification\n",
      "Arc Standard Model with all features plus the new feature REL\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8692365835222978\n",
      "Unlabelled attachment score:  0.8163265306122449\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Classification\n",
    "print('Logistic Regression Model used for Classification')\n",
    "# Arc Standard Model with all features plus the new feature REL\n",
    "print('Arc Standard Model with all features plus the new feature REL')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model used for Classification\n",
      "Arc Eager Model with all features plus the new feature REL\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8820861678004536\n",
      "Unlabelled attachment score:  0.8238851095993953\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Classification\n",
    "print('Logistic Regression Model used for Classification')\n",
    "# Arc Eager Model with all features plus the new feature REL\n",
    "print('Arc Eager Model with all features plus the new feature REL')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            '''\n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            '''\n",
    "            #model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Model used for Classification\n",
      "Arc Standard Model with all features plus the new feature REL\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.871504157218443\n",
      "Unlabelled attachment score:  0.8208616780045351\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "print('MLP Classifier Model used for Classification')\n",
    "# Arc Standard Model with all features plus the new feature REL\n",
    "print('Arc Standard Model with all features plus the new feature REL')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Model used for Classification\n",
      "Arc Eager Model with all features plus the new feature REL\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8843537414965986\n",
      "Unlabelled attachment score:  0.8276643990929705\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "print('MLP Classifier Model used for Classification')\n",
    "# Arc Eager Model with all features plus the new feature REL\n",
    "print('Arc Eager Model with all features plus the new feature REL')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class only for the feature REL\n",
    "class Configuration(object):\n",
    "    \"\"\"\n",
    "    Class for holding configuration which is the partial analysis of the input sentence.\n",
    "    The transition based parser aims at finding set of operators that transfer the initial\n",
    "    configuration to the terminal configuration.\n",
    "\n",
    "    The configuration includes:\n",
    "        - Stack: for storing partially proceeded words\n",
    "        - Buffer: for storing remaining input words\n",
    "        - Set of arcs: for storing partially built dependency tree\n",
    "\n",
    "    This class also provides a method to represent a configuration as list of features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dep_graph):\n",
    "        \"\"\"\n",
    "        :param dep_graph: the representation of an input in the form of dependency graph.\n",
    "        :type dep_graph: DependencyGraph where the dependencies are not specified.\n",
    "        \"\"\"\n",
    "        # dep_graph.nodes contain list of token for a sentence\n",
    "        self.stack = [0]  # The root element\n",
    "        self.buffer = list(range(1, len(dep_graph.nodes)))  # The rest is in the buffer\n",
    "        self.arcs = []  # empty set of arc\n",
    "        self._tokens = dep_graph.nodes\n",
    "        self._max_address = len(self.buffer)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Stack : ' + \\\n",
    "            str(self.stack) + '  Buffer : ' + str(self.buffer) + '   Arcs : ' + str(self.arcs)\n",
    "\n",
    "    def _check_informative(self, feat, flag=False):\n",
    "        \"\"\"\n",
    "        Check whether a feature is informative\n",
    "        The flag control whether \"_\" is informative or not\n",
    "        \"\"\"\n",
    "        if feat is None:\n",
    "            return False\n",
    "        if feat == '':\n",
    "            return False\n",
    "        if flag is False:\n",
    "            if feat == '_':\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def extract_features(self):\n",
    "        \"\"\"\n",
    "        Extract the set of features for the current configuration. Implement standard features as describe in\n",
    "        Table 3.2 (page 31) in Dependency Parsing book by Sandra Kubler, Ryan McDonal, Joakim Nivre.\n",
    "        Please note that these features are very basic.\n",
    "        :return: list(str)\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # Todo : can come up with more complicated features set for better\n",
    "        # performance.\n",
    "        if len(self.stack) > 0:\n",
    "            # Stack 0\n",
    "            stack_idx0 = self.stack[len(self.stack) - 1]\n",
    "            token = self._tokens[stack_idx0]\n",
    "            '''\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('STK_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('STK_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('STK_0_POS_' + token['tag'])\n",
    "            '''\n",
    "            if self._check_informative(token['rel']):\n",
    "                result.append('STK_0_REL_' + token['rel'])\n",
    "            '''\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('STK_0_FEATS_' + feat)\n",
    "            '''\n",
    "            # Stack 1\n",
    "            if len(self.stack) > 1:\n",
    "                stack_idx1 = self.stack[len(self.stack) - 2]\n",
    "                token = self._tokens[stack_idx1]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('STK_1_POS_' + token['tag'])\n",
    "\n",
    "            # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == stack_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('STK_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('STK_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        # Check Buffered 0\n",
    "        if len(self.buffer) > 0:\n",
    "            # Buffer 0\n",
    "            buffer_idx0 = self.buffer[0]\n",
    "            token = self._tokens[buffer_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('BUF_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('BUF_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('BUF_0_POS_' + token['tag'])\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('BUF_0_FEATS_' + feat)\n",
    "            # Buffer 1\n",
    "            if len(self.buffer) > 1:\n",
    "                buffer_idx1 = self.buffer[1]\n",
    "                token = self._tokens[buffer_idx1]\n",
    "                if self._check_informative(token['word'], True):\n",
    "                    result.append('BUF_1_FORM_' + token['word'])\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_1_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 2:\n",
    "                buffer_idx2 = self.buffer[2]\n",
    "                token = self._tokens[buffer_idx2]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_2_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 3:\n",
    "                buffer_idx3 = self.buffer[3]\n",
    "                token = self._tokens[buffer_idx3]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_3_POS_' + token['tag'])\n",
    "                    # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == buffer_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('BUF_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('BUF_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            \n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            \n",
    "            #model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            #model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model used for Classification\n",
      "Arc Standard Model with only REL feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8662131519274376\n",
      "Unlabelled attachment score:  0.8140589569160998\n"
     ]
    }
   ],
   "source": [
    "# SVM Model for Classification\n",
    "print('SVM Model used for Classification')\n",
    "# Arc Standard Model with only REL feature\n",
    "print('Arc Standard Model with only REL feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model used for Classification\n",
      "Arc Eager Model with only REL feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8465608465608465\n",
      "Unlabelled attachment score:  0.7906273620559335\n"
     ]
    }
   ],
   "source": [
    "# SVM Model for Classification\n",
    "print('SVM Model used for Classification')\n",
    "# Arc Eager Model with only REL feature\n",
    "print('Arc Eager Model with only REL feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            '''\n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            '''\n",
    "            model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            #model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model used for Classification\n",
      "Arc Standard Model with only REL feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8571428571428571\n",
      "Unlabelled attachment score:  0.7959183673469388\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Classification\n",
    "print('Logistic Regression Model used for Classification')\n",
    "# Arc Standard Model with only REL feature\n",
    "print('Arc Standard Model with only REL feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model used for Classification\n",
      "Arc Eager Model with only REL feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8246409674981103\n",
      "Unlabelled attachment score:  0.7679516250944822\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model for Classification\n",
    "print('Logistic Regression Model used for Classification')\n",
    "# Arc Eager Model with only REL feature\n",
    "print('Arc Eager Model with only REL feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPL Classifier Model for Classification\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "            '''\n",
    "            model = svm.SVC(\n",
    "                kernel='poly',\n",
    "                degree=2,\n",
    "                coef0=0,\n",
    "                gamma=0.2,\n",
    "                C=0.5,\n",
    "                verbose=verbose,\n",
    "                probability=True)\n",
    "            '''\n",
    "            #model=LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "            model=MLPClassifier(hidden_layer_sizes=(500, ), solver='lbfgs', learning_rate='adaptive', alpha=0.005)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                #dec_func = model.decision_function(x_test)[0]\n",
    "                #votes = {}\n",
    "                #k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                #sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Model used for Classification\n",
      "Arc Standard Model with only REL feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8805744520030234\n",
      "Unlabelled attachment score:  0.8269085411942555\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "print('MLP Classifier Model used for Classification')\n",
    "# Arc Standard Model with only REL feature\n",
    "print('Arc Standard Model with only REL feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-standard')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arcstd.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arcstd.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arcstd.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Model used for Classification\n",
      "Arc Eager Model with only REL feature\n",
      " Number of training examples : 500\n",
      " Number of valid (projective) examples : 476\n",
      "True\n",
      "Labelled attachment score:  0.8253968253968254\n",
      "Unlabelled attachment score:  0.7671957671957672\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier Model for Classification\n",
    "print('MLP Classifier Model used for Classification')\n",
    "# Arc Eager Model with only REL feature\n",
    "print('Arc Eager Model with only REL feature')\n",
    "# Temp Input File\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "# The Parser Standard \n",
    "parser_std = TransitionParser('arc-eager')\n",
    "# Training the Parser\n",
    "parser_std.train(f,'temp.arceager.model', verbose=False)\n",
    "# Removing the Temporary Input File\n",
    "remove(input_file.name)\n",
    "# Parsing the Test Data\n",
    "result = parser_std.parse(f1, 'temp.arceager.model')\n",
    "# Evaluvating the Result on the Test Data\n",
    "de = DependencyEvaluator(result, f1)\n",
    "# Just a check to see if everything worked properly\n",
    "print(de.eval() >= (0, 0))\n",
    "ans=de.eval()\n",
    "print('Labelled attachment score: ',ans[0])\n",
    "print('Unlabelled attachment score: ',ans[1])\n",
    "# Removing the Temporary Files\n",
    "remove('temp.arceager.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
